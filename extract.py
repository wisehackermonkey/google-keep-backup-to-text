# pulls out the text from a google keep backup 
# by oran collins
# github.com/wisehackermonkey
# oranbusiness@gmail.com
# 20200510
import json
from datetime import datetime

note_data = {
"color":"DEFAULT",
"isTrashed":false,
"isPinned":false,
"isArchived":false,
"textContent":"display paper text\nHolographic and lenslet displays rely on a two-dimensional (2D) display\nmodulator, constraining the visibility of 3D content to the volume\nbetween the observer\u2019s eyes and the display surface (that is, the direct\nline of sight). Volumetric approaches are based on light-scattering,\n-emitting or -absorbing surfaces9\n. They offer unconstrained visibility\nanywhere around the display and can be created using rotating surfaces\n(active1\n or passive2\n), plasmonics5,10, air displays11 and photophoretic\ntraps4\n. However, none of these approaches rely on operating principles\nthat can also recreate touch and sound. Acoustic levitation displays\nreported so far12\u201314 have only demonstrated control of a reduced number of points at reduced speeds and do not involve touch or audible\nsound. By contrast, our MATD provides a volumetric display, where\nusers can simultaneously see visual content in mid-air from any point\naround the display volume and receive auditive and tactile feedback\nfrom that volume (as shown in Supplementary Video 1).\nOur system is based on acoustic tweezers, which use ultrasound radiation forces to trap particles14\u201317. Trapping has been demonstrated in\nmedia such as air12,13,18,19 and water16 and for particle sizes ranging from the\nmicrometre to the centimetre scale. For spherical particles considerably\nsmaller than the wavelength and operating in the far-field regime (that\nis, such as those used by our MATD), the forces exerted are governed by\nthe gradient of the Gor\u2019kov potential17. Several trap morphologies have\nbeen demonstrated until now, including twin traps, vortex traps and\nbottle beams20\u201322, which can be analytically computed with efficiency22.\nOur device (illustrated in Fig. 1a and detailed in Methods) exploits\nthese advances, analytically computing single twin traps or focusing\npoints at a hardware level using a field-programmable gate array\n(FPGA). This enables position and amplitude updates of the trap in\na volume of 10 × 10 × 10 cm3\n, at a rate limited only by the transducer\nfrequency. By contrast, spatial light modulators are limited to update\nrates of hundreds of hertz, whereas galvanometers usually have update\nrates of up to about 20 kHz. Existing acoustic modulators are limited\nto hundreds of hertz14 and displacement speeds well below 1 m s−1. Our\ncurrent MATD implementation enables update rates of up to 40 kHz\nand particle displacement speeds of up to 8.75 m s−1 and 3.75 m s−1 in\nthe vertical and horizontal direction, respectively. By exploiting such\nhigh modulation rates and the mechanical nature of ultrasound, our\ncontrol techniques (described below and detailed in Methods) allow the\ndelivery of tactile and auditive content in addition to 3D persistenceof-vision (POV) content.\nOperating principles of the MATD\nTo create visual content, we levitate a 1-mm-radius, white, expanded\npolystyrene (EPS) particle as a good approximation to a Lambertian\nsurface. Such a particle allows the use of predictive models of acoustic trapping forces, as well as a simple analytical model to describe\nthe perceived colour under controlled illumination (see Methods,\n\u2018Illumination control\u2019). The hardware-embedded computation of the\ntwin trap (see Methods, \u2018Embedded computation of twin levitation\ntraps and focusing points\u2019) provides controlled and fast levitation of\nour scanning particle and is synchronized with a diffuse illumination\n\nmodule (RGB light-emitting diodes, LEDs). This allows the creation of\na POV display with accurate control of the perceived colour (gammacorrected with γ \u003d 2.2) that can deliver 2D or 3D vector contents by\nPOV (Fig. 1b, c, e) or fully rasterized contents (Fig. 1d; exposure time,\n20 s), even under conventional indoor illumination conditions (see\nSupplementary Video 4).\nOur tests (see Methods, \u2018Linear speed tests\u2019 and \u2018Acceleration, sharp\ncorners and minimum radius of curvature\u2019) revealed high scanning\nspeeds and accelerations, much higher than those of optical4\n or acoustic14 setups demonstrated until now. The most critical display parameters are summarized in Table 1 according to the various modes of\noperation of the MATD: single particle with no amplitude modulation\n(visual content only), single particle with minimum amplitude (in the\nworst-case scenario, displaying visual and audio content) and timemultiplexed dual trap with minimum amplitude (in the worst-case\nscenario, delivering all visual, audio and tactile content). The trapping\nforces and achievable speeds and accelerations vary with the direction of motion of the particle (that is, they are highest in the vertical\ndirection). Table 1 provides the maximum displacement parameters\nalong the horizontal direction (in the worst-case scenario, with weaker\ntrapping forces) as conservative reference values that allow content\nreproduction independently of the particle direction.\nThe parameters in Table 1 are used to compute and plan paths to\ncreate POV content visible to the naked eye. Human eyes can integrate\ndifferent light stimuli under a single percept (that is, a single shape or\ngeometry) during short periods of time (0.1 s is usually accepted as\na conservative estimation, even in bright environments23), and thus\nour particle needs to scan the content in less than this time (0.1 s).\nOur parameters allow us to determine feasible paths (particle speed,\nacceleration and curvature within the limits identified), which can be\nrevealed in less than 0.1 s by exploiting only a fraction of the display\u2019s\ncapabilities. The example letter in Fig. 1b (traced at 12.5 Hz, 1 × 2 cm2\n)\nrequires particle speeds of up to 0.8 m s−1, whereas the face and 3D torus\nknot in Fig. 1c (10 Hz, 1.8 cm diameter) and Fig. 1e (10 Hz, 2 cm side)\nrequire speeds of 1.3 m s−1. Our volumetric contents showed no substantial flicker and good colour reproduction (Fig. 2a), independently\nof the viewer\u2019s location (Fig. 3a, b). Figure 2a shows examples of colour\ntests performed with vector images (numbers, as in a seven-segment\ndisplay) and good colour saturation. Brighter images can be obtained\nby adding extra illumination modules or more powerful LEDs (details\nin Methods, \u2018Illumination control\u2019).\nFigure 2b shows the ability of the MATD to create additive and grayscale colours, and Figs. 1d, 2c, 3c show examples of raster colour content in two and three dimensions, similar to those created by Smalley\net al.4\n, using particle speeds of up to 0.6, 0.2 and 0.9 m s−1, respectively.\nThe effects of the particle scattering properties (that is, the perceived\ncolour around the particle), the particle speed (that is, the illuminance\naffected by the path length) and the human response (that is, nonlinear\nluminance response) must be considered for accurate colour reproduction (see Methods, \u2018Illumination control\u2019).\nMid-air tactile feedback at controlled locations (for example, the\nuser\u2019s hand) is created by using a secondary focusing trap and custom\nmultiplexing policy (position, but not amplitude, multiplexing with \n\nphase-difference minimization; details in Methods, \u2018Operational modes\nand multiplexing strategies for single and dual traps\u2019). Well differentiated tactile feedback was delivered using only a 25% duty cycle for\ntactile content. Thus, 75% of the cycles could still be used to position\nthe primary trap, and the tactile content resulted in minimum loss of\nscanning speed. For our experiments, we chose a modulation frequency\nof 250 Hz, avoiding the primary range of human auditive perception24\n(2 kHz\u20135 kHz) to minimize parasitic noise, but remaining well within\nthe optimum perceptual threshold of skin Lamellar corpuscles for\nvibration25. The 10-kHz update rate for tactile stimulation is sufficient\nfor spatio-temporal multiplexing strategies to maximize the fidelity of\nmid-air tactile content26. Our results (see Methods, \u2018Tactile generation\nand quality\u2019) show accurate positioning and focusing of the tactile\npoints and sound pressure levels greater than 150 dB, well above the\nthreshold of 72 dB required for tactile stimulation27 (illustrated in Supplementary Video 5).\nAudible sound is created by ultrasound demodulation using\nupper-sideband amplitude modulation28 of the traps. Our sampling\nat 40 kHz encodes most of the auditive spectrum (44.1 kHz), and the\nhigh-power transducer array produces audible sound even for a relatively small modulation index (a \u003d 0.2) while still modulating particle\npositions and tactile points at the 40-kHz rate. Figure 2a shows three\nexamples of visual content with simultaneous audible content of 60 dB.\nFor simultaneous auditive and tactile stimulation, we combine the\n40-kHz multifrequency audio signal with the tactile modulation signal (250 Hz), maintaining the sampling frequency of the individual\nsignals and reducing losses in audio quality (Supplementary Video 1).\nThe MATD supports two modes for audio generation (see Methods,\n\u2018Audio modes supported\u2019). The first mode uses the trapped particle\nas a scattering medium that implicitly provides spatialized audio29\n(that is, sound coming from the content displayed), but in our experience, such directional cues are weak (most of the sound comes from\nthe centre of our working volume). The second mode uses the secondary trap to steer sound towards the user, resulting into a stronger\ndirectional component and higher sound levels. However, the use\nof directional audio currently comes at the expense of delivering\nsimultaneous tactile feedback (simultaneous visual, tactile and directional audio would require multiplexing of three traps\u2014one for each\nmodality).\nPerformance of the MATD\nOur instantiation of the MATD presented here was created using lowcost, commercially available components, making it easy to reproduce \nbut also introducing limitations. Our tests were performed at a transducer voltage allowing continued usage (12 V peak to peak). Tests at\nhigher voltages (15 V peak to peak, duration \u003c1 h) indicate that increasing the transducer power can result in better performance parameters\n(for example, maximum horizontal speed of 4 m s−1) and more complex\ncontent. Increased power would also allow operation of the MATD at a\n50% duty cycle, further reducing audio artefacts (see Extended Data\nFig. 7d). Similarly, transducers operating at higher frequencies (that is,\n80 kHz) can also improve audio quality and, combined with a reduced\ntransducer pitch, would improve the spatial resolution of the levitation\ntraps (more accurate paths of the scanning particle).\nThe MATD demonstrated the possibility to manipulate particles by\nretaining them in a dynamic equilibrium (rather than a static one, as\nmost other levitation approaches; see Methods, \u2018Linear test speeds\u2019),\nenabling the high accelerations and speeds observed. The use of models\nthat accurately predict the dynamics of the particle (that is, in terms\nof acoustic forces, drag, gravity and centrifugal forces, but also considering interference from secondary traps and transient effects in\nthe transducer phase updates) would allow better exploitation of the\nobserved maximum speeds and accelerations, enabling larger and\nmore complex visual content. Alternatively, they could enable a more\nefficient use of the acoustic pressure, providing similar speeds and\naccelerations to those of the MATD, but allocating a lower duty cycle\nto the primary trap. This power could then be dedicated to achieving\nstronger tactile content or supporting a greater number of simultaneous traps (for example, the three traps required for the simultaneous\nvisual, tactile and directional audio scenario).\nMore advanced illumination approaches (for example, with galvanometers4\n or beam-steering mechanisms11) would allow the use of\nfocused light and brighter displays. The use of several illumination\nmodules around the display would provide greater control on the visual\nproperties of the content displayed. For instance, four illumination\nmodules, one at each corner of the MATD, would allow us to illuminate\nonly the outside part of the globe in Fig. 3c. The hidden parts of the\nglobe would be minimally visible, independently of the user location.\nCombining a denser illumination array (for example, a ring of light\nsources) and the predicted light-scattering pattern of our particle, the\ntotal scattered field from the particle can be computed as the linear\ncombination of the scattered fields from each light source. This could\nbe used, for instance, to create visual content approximating various\nmaterial properties (for example, to make content look metallic or\nmatte), simulating different lighting conditions or even delivering\ndifferent contents in different viewing directions.\nThe presence of the user\u2019s hands can distort the acoustic field\nowing to scattering from the hand\u2019s surface. The power and top\u2013down\narrangement of our array provide stable operation as the user\u2019s hand\napproaches from the sides or front (see Supplementary Video 4). Placing the hand below or above the location of the primary trap (occluding\none array) is much more likely to produce failures (that is, the scanning\nparticle being dropped). Close proximity of the secondary trap to the\nprimary trap can also distort the trapping of the scanning particle. We\nsuccessfully reproduced curvature tests at the maximum speed with the\ntactile point at 2 cm from the circle; this suggests that whereas tactile\nfeedback cannot be reproduced directly on top of the visual content (to\navoid scattering from or directly colliding with the scanning particle),\ntactile feedback can be created in close proximity to the visual content.\nOur study demonstrates an approach to creating volumetric POV\ndisplays with simultaneous delivery of auditive and tactile feedback and\nwith capabilities that exceed those of alternative optical approaches4\n.\nPolarization-based photophoretic approaches30 could match the\npotential for particle manipulation (that is, speeds and accelerations)\ndemonstrated in this study, but they would still be unable to include\nsound and touch. The demonstrated MATD prototype hence brings us\ncloser to volumetric displays providing a full sensorial reproduction\nof virtual content. Beyond opening a new avenue for multimodal 3D\ndisplays, our device and techniques enable positioning and amplitude\nmodulation of acoustic traps at the sound-field frequency rate (that is,\n40 kHz), providing also an interesting experimental setup for chemistry or laboratory-on-a-chip applications (for example, multi-particle\nlevitation and mode oscillations demonstrated in Extended Data Fig. 10\nand Supplementary Video 6).\nOnline content",
"title":"",
"userEditedTimestampUsec":1578193539618000}


print(note_data["title"])
print(note_data["textContent"])
print(datetime.fromtimestamp(note_data["userEditedTimestampUsec"]))
print(datetime.utcfromtimestamp(note_data["userEditedTimestampUsec"]))